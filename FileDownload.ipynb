{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('Phenotypic_V1_0b_preprocessed1.csv')\n",
    "\n",
    "# Define the output directory for saving the NIfTI files\n",
    "output_dir = 'nifti_files'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the numpy array\n",
    "num_samples = len(data)\n",
    "array = np.zeros((num_samples, 2), dtype=np.int32)\n",
    "matrices_par = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = 0\n",
    "# # Download and process each file\n",
    "# for i, row in data.iterrows():\n",
    "#     if x > 64 and x < 500:\n",
    "#         file_id = row['FILE_ID']\n",
    "#         dx_group = row['DX_GROUP']\n",
    "\n",
    "#         if file_id != 'no_filename':\n",
    "#             # Download the file\n",
    "#             url = f'https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Outputs/cpac/func_minimal/{file_id}_func_minimal.nii.gz'\n",
    "#             file_path = os.path.join(output_dir, f'{file_id}.nii.gz')\n",
    "#             urllib.request.urlretrieve(url, file_path)\n",
    "#         print(file_id, \" added\")\n",
    "#     x+=1\n",
    "# # Print the numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\joblib\\memory.py:349: FutureWarning: The default strategy for standardize is currently 'zscore' which incorrectly uses population std to calculate sample zscores. The new strategy 'zscore_sample' corrects this behavior by using the sample std. In release 0.13, the default strategy will be replaced by the new strategy and the 'zscore' option will be removed. Please use 'zscore_sample' instead.\n",
      "  return self.func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olin_0050110  added\n",
      "Olin_0050110  deleted\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "# Download and process each file\n",
    "for i, row in data.iterrows():\n",
    "    if x > 64 and x < 500:\n",
    "        file_id = row['FILE_ID']\n",
    "        dx_group = row['DX_GROUP']\n",
    "\n",
    "        if file_id != 'no_filename':\n",
    "            # Download the file\n",
    "            url = f'https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Outputs/cpac/func_minimal/{file_id}_func_minimal.nii.gz'\n",
    "#             file_path = os.path.join(output_dir, f'{file_id}.nii.gz')\n",
    "            urllib.request.urlretrieve(url, file_path)\n",
    "            image = nib.load(file_path)\n",
    "            image_data = image.get_fdata() \n",
    "            partial_correlation_matrix = partial_corr(image)\n",
    "            matrices_par.append(partial_correlation_matrix) \n",
    "            slice_order = np.arange(image.shape[2])\n",
    "\n",
    "            corrected_data = motion_correction(image_data)\n",
    "        print(file_id, \" added\")\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(file_id, \" deleted\")\n",
    "    x+=1\n",
    "# Print the numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "# Download and process each file\n",
    "for i, row in data.iterrows():\n",
    "    if x > 64 and x < 500:\n",
    "        file_id = row['FILE_ID']\n",
    "        dx_group = row['DX_GROUP']\n",
    "\n",
    "        if file_id != 'no_filename':\n",
    "            # Download the file\n",
    "            url = f'https://s3.amazonaws.com/fcp-indi/data/Projects/ABIDE_Initiative/Outputs/cpac/func_minimal/{file_id}_func_minimal.nii.gz'\n",
    "            file_path = os.path.join(output_dir, f'{file_id}.nii.gz')\n",
    "            urllib.request.urlretrieve(url, file_path)\n",
    "            print(file_id, \" added\")\n",
    "            image = nib.load(file_path)\n",
    "            partial_correlation_matrix = partial_corr(image)\n",
    "            slice_order = np.arange(image.shape[2])\n",
    "            slice_timing_cor= slice_timing_correction(image, slice_order, TR, TA)\n",
    "            corrected_data = motion_correction(slice_timing_cor)\n",
    "            matrices_par.append(partial_correlation_matrix)  \n",
    "            # Delete the downloaded file\n",
    "#             if os.path.exists(file_path):\n",
    "#                 os.remove(file_path)\n",
    "#                 print(file_id, \" deleted\")\n",
    "        x += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nibabel as nib\n",
    "from nilearn import image, input_data\n",
    "from scipy import stats\n",
    "\n",
    "def partial_corr(image_file):\n",
    "    # Load the atlas image\n",
    "    atlas_file = 'BN_Atlas_246_1mm.nii.gz'  # Replace with your atlas file\n",
    "    atlas_data = nib.load(atlas_file).get_fdata()\n",
    "\n",
    "    # Create a masker to extract ROI signals\n",
    "    masker = input_data.NiftiLabelsMasker(labels_img=atlas_file, standardize=True)\n",
    "\n",
    "    # Extract ROI signals from the image data\n",
    "    roi_signals = masker.fit_transform(image_file)\n",
    "    # Compute the partial correlation matrix\n",
    "    partial_corr_matrix = np.zeros((len(roi_signals.T), len(roi_signals.T)))\n",
    "    for i in range(len(roi_signals.T)):\n",
    "        for j in range(i+1, len(roi_signals.T)):\n",
    "            # Calculate the partial correlation coefficient\n",
    "            r_partial, _ = stats.pearsonr(roi_signals.T[i], roi_signals.T[j])\n",
    "\n",
    "            # Perform Fisher transformation to convert to a z-score\n",
    "            z_partial = 0.5 * np.log((1 + r_partial) / (1 - r_partial))\n",
    "\n",
    "            # Calculate the p-value\n",
    "            p_value = 2 * stats.norm.cdf(-np.abs(z_partial))\n",
    "\n",
    "            # Store the partial correlation coefficient in the matrix\n",
    "            partial_corr_matrix[i, j] = r_partial\n",
    "            partial_corr_matrix[j, i] = r_partial\n",
    "    return partial_corr_matrix\n",
    "\n",
    "# The partial correlation matrix will have shape (n_ROIs, n_ROIs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def slice_timing_correction(data, slice_order, TR, TA):\n",
    "    num_slices, num_rows, num_cols, num_timepoints = data.shape\n",
    "    corrected_data = np.zeros_like(data)\n",
    "\n",
    "    # Calculate the slice timing indices\n",
    "    slice_timings = np.arange(num_slices) * TR / num_slices\n",
    "\n",
    "    for t in range(num_timepoints):\n",
    "        for s in range(num_slices):\n",
    "            # Calculate the slice timing difference for each slice\n",
    "            timing_difference = slice_timings[s] - slice_order[s] * TA\n",
    "\n",
    "            # Interpolate the data for each slice using timing difference\n",
    "            interpolated_data = interpolate_slice(data[:, :, :, t], timing_difference)\n",
    "            corrected_data[:, :, s, t] = interpolated_data\n",
    "\n",
    "    return corrected_data\n",
    "\n",
    "def interpolate_slice(slice_data, timing_difference):\n",
    "    # Perform slice timing interpolation using linear interpolation\n",
    "    interp_func = interp1d(np.arange(slice_data.shape[2]), slice_data, axis=2, kind='linear', fill_value='extrapolate')\n",
    "    interpolated_data = interp_func(timing_difference)\n",
    "\n",
    "    return interpolated_data\n",
    "\n",
    "\n",
    "\n",
    "TR = 2.0\n",
    "TA = TR / len(slice_order)\n",
    "\n",
    "# Perform slice timing correction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def motion_correction(data):\n",
    "    num_slices, num_rows, num_cols, num_timepoints = data.shape\n",
    "    corrected_data = data.copy()\n",
    "\n",
    "    # Iterate over each timepoint\n",
    "    for t in range(num_timepoints):\n",
    "        # Generate random motion parameters (translations in x, y, z directions)\n",
    "        tx = random.uniform(-1, 1)\n",
    "        ty = random.uniform(-1, 1)\n",
    "        tz = random.uniform(-1, 1)\n",
    "\n",
    "        # Apply motion correction to each slice\n",
    "        for s in range(num_slices):\n",
    "            # Calculate the translation indices\n",
    "            tx_idx = int(round(tx * num_cols))\n",
    "            ty_idx = int(round(ty * num_rows))\n",
    "            tz_idx = int(round(tz * num_slices))\n",
    "\n",
    "            # Perform translation by shifting the slice\n",
    "            corrected_data[s, :, :, t] = translate_slice(data[s, :, :, t], tx_idx, ty_idx, tz_idx)\n",
    "\n",
    "    return corrected_data\n",
    "\n",
    "def translate_slice(slice_data, tx, ty, tz):\n",
    "    num_rows, num_cols = slice_data.shape\n",
    "    translated_slice = np.zeros_like(slice_data)\n",
    "\n",
    "    # Iterate over each pixel in the slice\n",
    "    for r in range(num_rows):\n",
    "        for c in range(num_cols):\n",
    "            # Apply translation to each pixel\n",
    "            new_r = r + ty\n",
    "            new_c = c + tx\n",
    "\n",
    "            # Check if the translated pixel is within the slice boundaries\n",
    "            if 0 <= new_r < num_rows and 0 <= new_c < num_cols:\n",
    "                translated_slice[new_r, new_c] = slice_data[r, c]\n",
    "\n",
    "    return translated_slice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
