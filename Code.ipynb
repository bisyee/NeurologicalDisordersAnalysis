{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\nilearn\\__init__.py:67: FutureWarning: Python 3.7 support is deprecated and will be removed in release 0.12 of Nilearn. Consider switching to Python 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\nilearn\\input_data\\__init__.py:23: FutureWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Implementations\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image, input_data\n",
    "from scipy import stats\n",
    "import networkx as nx\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import os      \n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "graph_attributes = np.empty((0, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nibabel as nib\n",
    "\n",
    "# Define the folder path where the .nii files are located\n",
    "folder_path = \"nifti_files\"\n",
    "\n",
    "# Use glob to get a list of .nii files in the folder\n",
    "nii_files = glob.glob(folder_path + \"/*.nii.gz\")\n",
    "\n",
    "# Iterate over the .nii files and read them using nibabel\n",
    "for nii_file in nii_files:\n",
    "    try:\n",
    "        # Load the NIfTI image\n",
    "        image = nib.load(nii_file)\n",
    "\n",
    "        # Process the image and calculate graph attributes\n",
    "        partial_correlation_matrix = partial_corr(image)\n",
    "        threshold_matrix = threshold(partial_correlation_matrix)\n",
    "        graph_attr = graph_attributesfunc(threshold_matrix)\n",
    "        communities_neki = communitiesnum(threshold_matrix)\n",
    "        graph_attr.append(communities_neki)\n",
    "        graph_attr.append(nii_file)\n",
    "        graph_attributes = np.append(graph_attributes, [graph_attr], axis=0)\n",
    "        embedding_modelar = embedding_model(threshold_matrix, nii_file)\n",
    "        print(graph_attr, \"added\")\n",
    "#[num_nodes, num_edges, avg_degree, lcc_percentage, avg_clustering]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {nii_file}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain parcellation and partial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_corr(image_file):\n",
    "    # Load the atlas image\n",
    "    atlas_file = 'BN_Atlas_246_1mm.nii.gz'  # Replace with your atlas file\n",
    "    atlas_data = nib.load(atlas_file).get_fdata()\n",
    "\n",
    "    # Create a masker to extract ROI signals\n",
    "    masker = input_data.NiftiLabelsMasker(labels_img=atlas_file, standardize=True)\n",
    "\n",
    "    # Extract ROI signals from the image data\n",
    "    roi_signals = masker.fit_transform(image_file)\n",
    "    # Compute the partial correlation matrix\n",
    "    partial_corr_matrix = np.zeros((len(roi_signals.T), len(roi_signals.T)))\n",
    "    for i in range(len(roi_signals.T)):\n",
    "        for j in range(i+1, len(roi_signals.T)):\n",
    "            # Calculate the partial correlation coefficient\n",
    "            r_partial, _ = stats.pearsonr(roi_signals.T[i], roi_signals.T[j])\n",
    "\n",
    "            # Perform Fisher transformation to convert to a z-score\n",
    "            z_partial = 0.5 * np.log((1 + r_partial) / (1 - r_partial))\n",
    "\n",
    "            # Calculate the p-value\n",
    "            p_value = 2 * stats.norm.cdf(-np.abs(z_partial))\n",
    "\n",
    "            # Store the partial correlation coefficient in the matrix\n",
    "            partial_corr_matrix[i, j] = r_partial\n",
    "            partial_corr_matrix[j, i] = r_partial\n",
    "    return partial_corr_matrix\n",
    "\n",
    "# The partial correlation matrix will have shape (n_ROIs, n_ROIs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(partial_corr_matrix):\n",
    "    # Thresholding parameters\n",
    "    mean = np.mean(partial_corr_matrix)\n",
    "    std = np.std(partial_corr_matrix)\n",
    "    # Set the threshold value as a multiple of the standard deviation\n",
    "    threshold_value2 = mean + std\n",
    "    threshold_value1 = 4.0 # Set the threshold value as per your requirement\n",
    "    # Thresholding the correlation matrix\n",
    "    adjacency_matrix = np.where(np.abs(partial_corr_matrix) >= threshold_value2, 1, 0)  # Binary adjacency matrix\n",
    "    weighted_adjacency_matrix = np.where(np.abs(partial_corr_matrix) >= threshold_value2, np.abs(partial_corr_matrix), 0)  # Weighted adjacency matrix\n",
    "    return weighted_adjacency_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  graphmod(weighted_adjacency_matrix):\n",
    "    graph = nx.from_numpy_array(weighted_adjacency_matrix)\n",
    "    return graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_attributesfunc(weighted_adjacency_matrix):\n",
    "    # Create a graph from the weighted adjacency matrix\n",
    "    graph = nx.from_numpy_array(weighted_adjacency_matrix)\n",
    "\n",
    "    # Optional: Set the labels of the nodes if available\n",
    "    node_labels = range(weighted_adjacency_matrix.shape[0])  # Assuming each row/column represents a node\n",
    "    labels = {i: label for i, label in enumerate(node_labels)}\n",
    "    nx.set_node_attributes(graph, labels, 'label')\n",
    "    # Optional: Set the edge weights as attributes\n",
    "    edge_weights = {(u, v): weight for (u, v, weight) in graph.edges(data='weight')}\n",
    "    nx.set_edge_attributes(graph, edge_weights, 'weight')\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "\n",
    "    # Calculate the average degree\n",
    "    avg_degree = sum(dict(graph.degree()).values()) / num_nodes\n",
    "\n",
    "    # Calculate the largest connected component (LCC)\n",
    "    lcc = max(nx.connected_components(graph), key=len)\n",
    "    lcc_percentage = len(lcc) / num_nodes * 100\n",
    "    # Calculate the clustering coefficient\n",
    "    avg_clustering = nx.average_clustering(graph)\n",
    "        # Calculate degree mixing\n",
    "    degree_mixing = nx.degree_mixing_dict(graph)\n",
    "\n",
    "    # Calculate degree assortativity\n",
    "    degree_assortativity = nx.degree_assortativity_coefficient(graph, x='out', y='in')\n",
    "    return [num_nodes, num_edges, avg_degree, lcc_percentage, avg_clustering, degree_assortativity]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communitiesnum(weighted_adjacency_matrix):\n",
    "    graph = nx.from_numpy_array(weighted_adjacency_matrix)\n",
    "    \n",
    "    # Perform community detection using the Louvain algorithm\n",
    "    partition = community.best_partition(graph)\n",
    "\n",
    "    # Create a dictionary of nodes and their corresponding community IDs\n",
    "    communities = {}\n",
    "    for node, comm_id in partition.items():\n",
    "        communities[node] = comm_id\n",
    "\n",
    "    # Create a subgraph for each community\n",
    "    community_subgraphs = []\n",
    "    for community_id in set(communities.values()):\n",
    "        nodes_in_community = [node for node, comm_id in communities.items() if comm_id == community_id]\n",
    "        community_subgraph = graph.subgraph(nodes_in_community)\n",
    "        community_subgraphs.append(community_subgraph)\n",
    "\n",
    "    # Find the giant component (largest connected component)\n",
    "    giant_component = max(community_subgraphs, key=len)\n",
    "    community_subgraphs.append(giant_component)\n",
    "\n",
    "    return len(community_subgraphs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nodevectors import Node2Vec\n",
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit embedding model to graph\n",
    "g2v = Node2Vec(\n",
    "    n_components=32,\n",
    "    walklen=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the edge weights to object data type\n",
    "edge_weights = {(u, v): float(weight) for u, v, weight in graph.edges(data='weight')}\n",
    "nx.set_edge_attributes(graph, edge_weights, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_model(weighted_adjacency_matrix, nii_file):\n",
    "    # Create a graph from the weighted adjacency matrix\n",
    "    graph = nx.from_numpy_array(weighted_adjacency_matrix)\n",
    "    # Preprocess the graph for node2vec\n",
    "    node2vec = Node2Vec(graph)\n",
    "    # Generate walks\n",
    "    walks = node2vec.walks\n",
    "    # Train the node2vec model on the walks\n",
    "    model = node2vec.fit()\n",
    "    # Get the embeddings for all nodes\n",
    "    embeddings = {str(node): model.wv[str(node)] for node in graph.nodes}\n",
    "    # Query embeddings for node 42\n",
    "    embedding_42 = embeddings[str(42)]\n",
    "    filename = str(nii_file) + '_node2vec_embeddings.txt'\n",
    "    # Save the embeddings to a file\n",
    "    model.wv.save_word2vec_format(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "folder_path = \"nifti_files\"  # Path to the folder containing the embedding files\n",
    "embedding_dimension = 128  # Embedding dimension size\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter files with the naming convention \"patient_id_node2vec_embeddings\"\n",
    "embedding_files = [file for file in file_list if \"_node2vec_embeddings\" in file]\n",
    "\n",
    "# Initialize an empty matrix to store the embeddings\n",
    "embedding_matrix = np.zeros((len(embedding_files), embedding_dimension))\n",
    "\n",
    "# Iterate over each embedding file\n",
    "for i, file in enumerate(embedding_files):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Extract the embedding vectors (excluding the first line)\n",
    "    embedding_vectors = []\n",
    "    for line in lines[1:]:\n",
    "        vector = list(map(float, line.split()[1:]))\n",
    "        embedding_vectors.append(vector)\n",
    "    \n",
    "    # Calculate the average embedding vector\n",
    "    average_embedding = np.mean(embedding_vectors, axis=0)\n",
    "    \n",
    "    # Store the average embedding vector in the embedding matrix\n",
    "    embedding_matrix[i] = average_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.8333333333333334\n",
      "F1-score: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Perform oversampling on the embedding matrix and labels\n",
    "oversampler = RandomOverSampler()\n",
    "labels = np.array([1,1,1,1,1,1,1, 1, 0, 0,0, 0, 1, 1, 1, 1,1,1,1])  # Example labels, replace with your actual labels\n",
    "embedding_matrix_resampled, labels_resampled = oversampler.fit_resample(embedding_matrix, labels)\n",
    "\n",
    "# Split the resampled data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedding_matrix_resampled, labels_resampled, test_size=0.33)\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = SVC()\n",
    "\n",
    "# Train the model on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "\n",
    "\n",
    "# Create the predicted edge list\n",
    "predicted_edge_list = []\n",
    "for i, y in enumerate(y_pred):\n",
    "    if y == 1:\n",
    "        source_node = i  # Assuming that the node indices correspond to the row index in the embedding_matrix\n",
    "        for j, true_y in enumerate(y_test):\n",
    "            if true_y == 1 and j != i:\n",
    "                target_node = j\n",
    "                predicted_edge_list.append((source_node, target_node, 1.0))  # Use a constant weight of 1.0 for predicted edges\n",
    "\n",
    "# true_network_graph = nx.from_numpy_array(threshold_matrix)\n",
    "# # Compute the Mean Average Precision (MAP)\n",
    "# map_val = computeMAP(predicted_edge_list, true_network_graph)  # Replace 'true_network_graph' with the actual network graph object\n",
    "\n",
    "# print(\"MAP:\", map_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Accuracy: 0.75\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 1.0\n",
      "F1-score: 0.8\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# ... (previous cod\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedding_matrix_resampled, labels_resampled, test_size=0.25)\n",
    "# Initialize the Logistic Regression classifier\n",
    "logreg_classifier = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set using Logistic Regression\n",
    "logreg_y_pred = logreg_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score using Logistic Regression\n",
    "logreg_accuracy = accuracy_score(y_test, logreg_y_pred)\n",
    "logreg_precision = precision_score(y_test, logreg_y_pred)\n",
    "logreg_recall = recall_score(y_test, logreg_y_pred)\n",
    "logreg_f1 = f1_score(y_test, logreg_y_pred)\n",
    "\n",
    "# Calculate ROC-AUC score using Logistic Regression\n",
    "logreg_roc_auc = roc_auc_score(y_test, logreg_y_pred)\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(\"Accuracy:\", logreg_accuracy)\n",
    "print(\"Precision:\", logreg_precision)\n",
    "print(\"Recall:\", logreg_recall)\n",
    "print(\"F1-score:\", logreg_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "folder_path = \"nifti_files\"  # Path to the folder containing the embedding files\n",
    "embedding_dimension = 128  # Embedding dimension size\n",
    "\n",
    "# Get a list of all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Filter files with the naming convention \"patient_id_node2vec_embeddings\"\n",
    "embedding_files = [file for file in file_list if \"_node2vec_embeddings\" in file]\n",
    "\n",
    "# Initialize an empty matrix to store the embeddings\n",
    "embedding_matrix = np.zeros((len(embedding_files), embedding_dimension))\n",
    "\n",
    "# Iterate over each embedding file\n",
    "for i, file in enumerate(embedding_files):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Extract the embedding vectors (excluding the first line)\n",
    "    embedding_vectors = []\n",
    "    for line in lines[1:]:\n",
    "        vector = list(map(float, line.split()[1:]))\n",
    "        embedding_vectors.append(vector)\n",
    "    \n",
    "    # Calculate the average embedding vector\n",
    "    average_embedding = np.mean(embedding_vectors, axis=0)\n",
    "    \n",
    "    # Store the average embedding vector in the embedding matrix\n",
    "    embedding_matrix[i] = average_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  # Example classifier, replace with your desired algorithm\n",
    "\n",
    "labels = np.array([1,1,1,1,1,1,1, 1, 1, 0,0, 0, 1, 1, 1, 1,1,1,1])  # Example labels, replace with your actual labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedding_matrix, labels, test_size=0.33)\n",
    "\n",
    "\n",
    "classifier = SVC()  # Initialize the classifier\n",
    "classifier.fit(X_train, y_train)  # Train the model on the training data\n",
    "accuracy = classifier.score(X_test, y_test)  # Calculate the accuracy on the test set\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7142857142857143\n",
      "Recall: 1.0\n",
      "F1 Score: 0.8333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Your code for loading data, splitting, and training the classifier\n",
    "\n",
    "# Calculate predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OHSU_0050142.nii.gz',\n",
       " 'OHSU_0050142.nii.gz_node2vec_embeddings.txt',\n",
       " 'OHSU_0050143.nii.gz',\n",
       " 'OHSU_0050143.nii.gz_node2vec_embeddings.txt',\n",
       " 'OHSU_0050144.nii.gz',\n",
       " 'OHSU_0050144.nii.gz_node2vec_embeddings.txt',\n",
       " 'OHSU_0050145.nii.gz',\n",
       " 'OHSU_0050145.nii.gz_node2vec_embeddings.txt',\n",
       " 'OHSU_0050146.nii.gz',\n",
       " 'OHSU_0050146.nii.gz_node2vec_embeddings.txt',\n",
       " 'OHSU_0050147.nii.gz',\n",
       " 'OHSU_0050147.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'OHSU_0050148.nii.gz',\n",
       " 'OHSU_0050148.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'OHSU_0050149.nii.gz',\n",
       " 'OHSU_0050150.nii.gz',\n",
       " 'OHSU_0050152.nii.gz',\n",
       " 'Olin_0050110.nii.gz',\n",
       " 'Olin_0050110.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050111.nii.gz',\n",
       " 'Olin_0050111.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050112.nii.gz',\n",
       " 'Olin_0050113.nii.gz',\n",
       " 'Olin_0050113.nii.gz_node2vec_embeddings.txt',\n",
       " 'Olin_0050114.nii.gz',\n",
       " 'Olin_0050126.nii.gz',\n",
       " 'Olin_0050127.nii.gz',\n",
       " 'Olin_0050128.nii.gz',\n",
       " 'Olin_0050128.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050129.nii.gz',\n",
       " 'Olin_0050129.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050130.nii.gz',\n",
       " 'Olin_0050130.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050131.nii.gz',\n",
       " 'Olin_0050131.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050132.nii.gz',\n",
       " 'Olin_0050132.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050133.nii.gz',\n",
       " 'Olin_0050133.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050134.nii.gz',\n",
       " 'Olin_0050134.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050135.nii.gz',\n",
       " 'Olin_0050135.nii.gz_1_node2vec_embeddings.txt',\n",
       " 'Olin_0050136.nii.gz',\n",
       " 'Olin_0050136.nii.gz_node2vec_embeddings.txt',\n",
       " 'Pitt_0050003_func_minimal.nii']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
